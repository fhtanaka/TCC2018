\chapter{Embasamento Teórico (nome temporario)}
\label{cap:embasamento_teorico}
\section{Aprendizado por Reforço}
\quad Aprendizado por reforço é um campo do aprendizado de máquina onde um
agente aprende como interagir com o ambiente por meio de tentativa e erro, 
isso é, aplicando ações e observando os resultados.\\

\quad Em cada passo o agente recebe do ambiente o estado em que se encontra e
então seleciona uma ação para  tomar. Após isso o ambiente retorna dois
valores: o novo estado que o agente está e uma recompensa, que pode ser
tanto positiva quanto negativa, que avalia o quão boa foi a ação realizada. Este
processo então é repetido até que o ambiente devolva um estado terminal e assim
finalize o episódio.\\

\quad O modelo acima é formalizado matematicamente utilizando um MDP 
(\textit{Markov Decision Process})

\subsection{Markov Decision Process}
\quad Um MDP é um processo estocástico onde  a distribuição de probabilidade do
próximo estado depende apenas do estado atual e não na sequência de eventos
que  o precederam. Esta característica é chamada de Markoviana e nela só se interessa
o estado imediato. O aprendizado por reforço pode ser modelado utilizando um MDP 
da seguinte forma:
 
 \begin{itemize}
 	\item Um conjunto de estados $S$ tal que cada estado $s_{i} \in S$
 	\item Um conjunto de ações $A$ tal que cada ação $a_{i} \in A$
 	\item Uma função de recompensa $R$ que recebe como entrada um par (estado, ação)
 	\item Uma função P de probabilidade de transição dado um estado $s$ e uma ação $a$
 \end{itemize}
 \newpage

\quad A imagem a seguir ilustra o funcionamento deste modelo:
\begin{figure}[htb]
\centering
\includegraphics[width=12cm]{figuras/reinforcement_learning}
\caption{\label{fig:reinforcement_learning}Substituir por imagem pt BR}
\end{figure}

\quad A partir destas definições pode-se definir também uma política $\pi$ ,
isso é, uma função que dado um certo estado devolve a ação que se deve tomar.
Com conhecimento destes conceitos o modelo é iterado da seguinte forma :

\begin{algorithm}[]
 Comece por um estado inicial $s_{0}$ e com tempo $t=0$\;
 \Enqto {$s_{t}$ não for terminal}{
	Selecione uma ação $a_{t}$ a partir da política $\pi$\;
	Realize a ação $a_{t}$ com probabilidade $P(s_{t}, a_{t})$\;
	Ambiente calcula a recompensa $r_{t} = R(s_{t}, a_{t})$\;
	Ambiente calcula o novo estado $s_{t+1}$\;
	Agente recebe o estado $s_{t+1}$ e a recompensa $r_{t}$\;
}
\end{algorithm}

\quad O objetivo deste modelo é maximizar a recompensa acumulada 
$\sum_{t=0} \gamma^{t}r_{t}$ onde $\gamma$ é um fator de desconto 
entre 0 e 1 que indica o quão mais importante são as recompensas imediatas.
A partir disso o valor de certa política $\pi$ em um estado $s$ pode 
ser definido como:
\begin{center}
$V_{\pi}(s) = E[\sum\limits_{t=0} \gamma^{t}r_{t}|s_{0}=s, \pi]$\\
\end{center}

E portanto busca-se uma política $\pi^{*}$ que maximize esse valor para um 
estado inicial.

\subsection{Q-learning}
\quad Em Q-learning é importante definir o valor de uma ação $a_{t}$ em dado 
estado $s_{t}$ e para isso é utilizada a seguinte função:
\begin{center}
$Q^{\pi}(s_{t},a_{t})=E[\sum\limits_{t=0} \gamma^{t}r_{t}|s_{0}=s_{t}, a_{0}=a_{t}, \pi]$
\end{center}

\quad Quando essa função $Q$ é ótima a chamamos de $Q^{*}$ e ela satisfaz à 
função de Bellman:
\begin{center}
$Q^{*}(s_{t},a_{t})=E[r_{t} + \gamma max_{a} Q^{*}(s_{t+1},a)]$
\end{center}

\quad A intuição para esta equação é simples, se temos uma política ótima, 
qualquer que seja o estado $s_{t}$ será selecionada a melhor ação possível, 
logo o valor deste estado é a recompensa desta ação somado ao valor do
próximo estado levando em consideração o desconto $\gamma$. Como a política
é ótima é considerado que neste próximo estado também será selecionada a melhor 
ação.\\

\quad Esta função é recursiva e pode-se a usar para iterar sobre a função Q para aproxima-la
de $Q^{*}$ da seguinte forma: $Q(s_{t},a_{t})\leftarrow r_{t} + \gamma max_{a} Q(s_{t+1},a)]$.\\

\quad O problema desta abordagem é que é necessário mapear o valor de todos
os estados e isso é muito ineficiente quando se trata de espaços muitos grandes.
No Hex, por exemplo, mesmo um tabuleiro 5x5 que é considerado pequeno tem 
847288609443 estados possíveis já que cada espaço do tabuleiro pode ter 1 dos
três possíveis valores: vazio, ocupado por peça preta e ocupado por peça branca.
Devido a esse problema são utilizada redes neurais profundas para realizar o 
aprendizado.

\section{Aprendizado por Reforço Profundo}
\quad Aprendizado profundo é um campo do aprendizado de máquina que tem como
objetivo extrair características (features) diretamente dos dados. Para isso ele
se baseia em abstrações de alto nível que utilizam grafos para representar 
as várias camadas de processamento. Neste projeto serão utilizadas redes neurais
convolucionais (CNNs) como a arquitetura de aprendizado profundo.

\subsection{Redes Neurais}
\quad Antes de se aprofundar em CNNs é importante ter conhecimento do
funcionamento de redes neurais.
