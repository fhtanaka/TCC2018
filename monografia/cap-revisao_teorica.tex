\chapter{Revisão Teorica}
\label{cap:revisao_teorica}
\section{Aprendizado por Reforço}
\quad Aprendizado por reforço é um campo do aprendizado de máquina onde um
agente aprende como interagir com o ambiente por meio de tentativa e erro, 
isso é, aplicando ações e observando os resultados.\\

\quad Em cada passo o agente recebe do ambiente o estado em que se encontra e
então seleciona uma ação para  tomar. Após isso o ambiente retorna dois
valores: o novo estado que o agente está e uma recompensa, que pode ser
tanto positiva quanto negativa, que avalia o quão boa foi a ação realizada. Este
processo então é repetido até que o ambiente devolva um estado terminal e assim
finalize o episódio.\\

\quad O modelo acima é formalizado matematicamente utilizando um MDP 
(\textit{Markov Decision Process})

\subsection{Processos de Decisão Markovianos  (MDP)}
\quad Um MDP (\textit{Markov Decision Process})  é um processo estocástico onde  a distribuição de probabilidade do
próximo estado depende apenas do estado atual e não na sequência de eventos
que  o precederam. Esta característica é chamada de Markoviana e nela só se interessa
o estado imediato. O aprendizado por reforço pode ser modelado utilizando um MDP 
da seguinte forma:
 
 \begin{itemize}
 	\item Um conjunto de estados $S$ tal que cada estado $s_{i} \in S$
 	\item Um conjunto de ações $A$ tal que cada ação $a_{i} \in A$
 	\item Uma função de recompensa $R$ que recebe como entrada um par (estado, ação)
 	\item Uma função P de probabilidade de transição dado um estado $s$ e uma ação $a$
 \end{itemize}
 \newpage

\quad A imagem a seguir ilustra o funcionamento deste modelo:
\begin{figure}[htb]
\centering
\includegraphics[width=12cm]{figuras/reinforcement_learning}
\caption{\label{fig:reinforcement_learning}Substituir por imagem pt BR}
\end{figure}

\quad A partir destas definições pode-se definir também uma política $\pi$ ,
isso é, uma função que dado um certo estado devolve a ação que se deve tomar.
Com conhecimento destes conceitos o modelo é iterado da seguinte forma :

\begin{algorithm}[]
 Comece por um estado inicial $s_{0}$ e com tempo $t=0$\;
 \Enqto {$s_{t}$ não for terminal}{
	Selecione uma ação $a_{t}$ a partir da política $\pi$\;
	Realize a ação $a_{t}$ com probabilidade $P(s_{t}, a_{t})$\;
	Ambiente calcula a recompensa $r_{t} = R(s_{t}, a_{t})$\;
	Ambiente calcula o novo estado $s_{t+1}$\;
	Agente recebe o estado $s_{t+1}$ e a recompensa $r_{t}$\;
}
\end{algorithm}

\quad O objetivo deste modelo é maximizar a recompensa acumulada 
$\sum_{t=0} \gamma^{t}r_{t}$ onde $\gamma$ é um fator de desconto 
entre 0 e 1 que indica o quão mais importante são as recompensas imediatas.
A partir disso o valor de certa política $\pi$ em um estado $s$ pode 
ser definido como:
\begin{center}
$V_{\pi}(s) = E[\sum\limits_{t=0} \gamma^{t}r_{t}|s_{0}=s, \pi]$\\
\end{center}

E portanto busca-se uma política $\pi^{*}$ que maximize esse valor para um 
estado inicial.

\subsection{Q-learning}
\quad Em Q-learning é importante definir o valor de uma ação $a_{t}$ em dado 
estado $s_{t}$ e para isso é utilizada a seguinte função:
\begin{center}
$Q^{\pi}(s_{t},a_{t})=E[\sum\limits_{t=0} \gamma^{t}r_{t}|s_{0}=s_{t}, a_{0}=a_{t}, \pi]$
\end{center}

\quad Quando essa função $Q$ é ótima a chamamos de $Q^{*}$ e ela satisfaz à 
função de Bellman:
\begin{center}
$Q^{*}(s_{t},a_{t})=E[r_{t} + \gamma max_{a} Q^{*}(s_{t+1},a)]$
\end{center}

\quad A intuição para esta equação é simples, se temos uma política ótima, 
qualquer que seja o estado $s_{t}$ será selecionada a melhor ação possível, 
logo o valor deste estado é a recompensa desta ação somado ao valor do
próximo estado levando em consideração o desconto $\gamma$. Como a política
é ótima é considerado que neste próximo estado também será selecionada a melhor 
ação.\\

\quad Esta função é recursiva e pode-se a usar para iterar sobre a função Q para aproxima-la
de $Q^{*}$ da seguinte forma: $Q(s_{t},a_{t})\leftarrow r_{t} + \gamma max_{a} Q(s_{t+1},a)]$.\\

\quad O problema desta abordagem é que é necessário mapear o valor de todos
os estados e isso é muito ineficiente quando se trata de espaços muitos grandes.
No Hex, por exemplo, mesmo um tabuleiro 5x5 que é considerado pequeno tem 
847288609443 estados possíveis já que cada espaço do tabuleiro pode ter 1 dos
três possíveis valores: vazio, ocupado por peça preta e ocupado por peça branca.
Devido a esse problema são utilizada redes neurais profundas para realizar o 
aprendizado.

\section{Aprendizado por Reforço Profundo}
\quad Aprendizado profundo é um campo do aprendizado de máquina que tem como
objetivo extrair características (features) diretamente dos dados. Para isso ele
se baseia em abstrações de alto nível que utilizam grafos para representar 
as várias camadas de processamento. Neste projeto serão utilizadas redes neurais
convolucionais como a arquitetura de aprendizado profundo.

\subsection{Redes Neurais Artificiais}

\quad Uma rede neural artificial  (\textit{Artificial Neural Networks} - ANNs) 
é um modelo computacional inspirado pelo funcionamento do cérebro no reino animal.
A ideia deste sistema é que a partir de exemplos seja possível "aprender" e assim realizar tarefas.\\

\quad Uma ANN é composta por unidades menores chamadas de neurônios que recebem
um vetor de valores $X$, os multiplica por um vetor de pesos $W$ e os soma. Após isso é
adicionado um viés $b$ e aplicada uma função de ativação $\theta$. Ou seja, o 
neurônio realiza a operação $\theta (W^{T}X + b)$ e devolve o resultado. 
Estas unidades  criam uma rede por camadas tal que a entrada
de dado nível é a saída da camada anterior. 

\begin{figure}[H]
  \centering
  \subfloat[Neurônio]{\includegraphics[width=0.4\textwidth]{neuron.png}\label{neuron}}
  \hfill
  \subfloat[Exemplo de ANN]{\includegraphics[width=0.4\textwidth]{ANN.jpeg}\label{ANN}}
  \caption{Trocar por imagens originais.}
\end{figure}

\quad Esta rede então é treinada utilizando exemplos, isso é, seus pesos ($W$) e viés ($b$)
são modificados com o intuito de diminuir o valor da função de erro que é calculada
a partir da diferença entre a saida esperada e a saida da rede. Normalmente o algoritmo
chamado de \textit{Backpropagation} é utilizado para alterar estes valores de forma a 
se aproximar do mínimo de erro possível. 

\quad O processo conhecido como aprendizado profundo (\textit{Deep Learning}) é o
uso de ANNs com muitas camadas ocultas para assim modelar abstrações
mais complexas.

\subsection{Redes Neurais Convolucionais}

\quad Redes neurais convolucionais (\textit{Convolutional Neural Networks} - CNNs) 
são uma classe de ANNs do tipo \textit{feed forward} (quando as conexões entre 
os neurônios não formam ciclos) normalmente utilizadas para a análise de imagens.
O intuito de uma CNN é extrair e mapear caractrísticas da entrada para facilitar
o processamento dos dados.\\

\quad Na construção de uma CNN são realizadas 4 operações: convolução, aplicação
de função não-linear (ReLU), \textit{pooling} e classificação. As 3 primeiras delas
formam uma camada oculta (\textit{hidden layer}) e podem ser utilizadas repetidas
vezes na construção do modelo.\\

\quad A convolução é o processo que dá nome à rede e nela a ideia é extrair informação
dos dados por meio da aplicação de um filtro (também chamado de \textit{kernel}).
A operação é feita deslizando o filtro sobre a matriz de entrada e em cada posição
os valores são multiplicados e então somados de forma a criar um novo valor
no que é chamado de \textit{feature map}. Este processo pode ser visualizado na
imagem abaixo.

\begin{figure}[htb]
\centering
\includegraphics[width=12cm]{figuras/convolution}
\caption{\label{fig:convolution}Alterar legenda}
\end{figure}

\quad Após a convolução é aplicado em cada elemento uma função
chamada de ReLU (\textit{Rectified Linear Unit}) tal que $ReLU(x) = max(0, x)$.
Esta operação troca qualquer valor negativo por 0 e tem como objetivo tornar
o dado não-linear pois assim ele fica mais próximo da realidade.\\

\quad O processo de \textit{pooling} reduz a dimensão do \textit{feature map} tentando
manter, ainda assim, as informações relevantes. O tipo mais comum é o \textit{max pooling}
que seleciona o maior valor de certo intervalo do dado. O objetivo desta parte é
diminuir o tamanho da amostra para agilizar o treinamento e prevenir o \textit{overfitting}.\\

\quad Por fim é realizada a classificação do dado. Após a entrada ser processada pelas
camadas anteriores o resultado é analisado e classificado por uma ANN.